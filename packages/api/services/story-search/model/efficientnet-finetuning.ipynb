{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# EfficientNet-B0 Fine-tuning for Comic Reverse Image Search\n",
        "\n",
        "This notebook fine-tunes EfficientNet-B0 for comic reverse image search using triplet loss.\n",
        "\n",
        "**Key advantages of EfficientNet-B0:**\n",
        "- **Small size**: ~18MB ONNX (vs 331MB for ViT-Base)\n",
        "- **High quality**: 0.136 similarity separation in our tests\n",
        "- **Fast inference**: ~264ms per batch\n",
        "- **Proven architecture**: EfficientNet is well-established\n",
        "\n",
        "**Training approach:**\n",
        "- Uses triplet loss for similarity learning\n",
        "- Pre-trained on ImageNet for better initialization\n",
        "- Optimized transforms for EfficientNet\n",
        "- Mixed precision training for efficiency\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install torch torchvision pytorch-lightning timm matplotlib tqdm --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import TripletMarginLoss\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import pytorch_lightning as pl\n",
        "import timm\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "pl.seed_everything(42)\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EfficientNetEmbeddingModel(pl.LightningModule):\n",
        "    def __init__(self, model_name=\"efficientnet_b0\", lr=1e-4, embed_dim=512):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        \n",
        "        # Load EfficientNet-B0 backbone\n",
        "        self.backbone = timm.create_model(model_name, pretrained=True, num_classes=0)  # Remove classifier\n",
        "        self.feature_dim = self.backbone.num_features  # 1280 for EfficientNet-B0\n",
        "        \n",
        "        # Projection head\n",
        "        self.fc = nn.Linear(self.feature_dim, embed_dim)\n",
        "        \n",
        "        # Loss function\n",
        "        self.loss_fn = TripletMarginLoss(margin=0.2)\n",
        "        self.training_losses = []\n",
        "        \n",
        "        print(f\"EfficientNet-B0: feature_dim={self.feature_dim}, embed_dim={embed_dim}\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)\n",
        "        embeddings = self.fc(features)\n",
        "        return F.normalize(embeddings, p=2, dim=1)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        anchor, positive, negative = batch\n",
        "        emb_a = self(anchor)\n",
        "        emb_p = self(positive)\n",
        "        emb_n = self(negative)\n",
        "        loss = self.loss_fn(emb_a, emb_p, emb_n)\n",
        "        self.training_losses.append(loss.item())\n",
        "        self.log(\"train_loss\", loss, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.AdamW(self.parameters(), lr=self.hparams.lr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset loading functions (same as original)\n",
        "def get_image_files(root_dir, batch_cache=10000):\n",
        "    \"\"\"Fast, cached file indexing\"\"\"\n",
        "    print(f\"Scanning for image files in {root_dir} ...\")\n",
        "    exts = {'.jpg', '.jpeg', '.png'}\n",
        "    all_files = []\n",
        "\n",
        "    root_path = Path(root_dir)\n",
        "    for cls_dir in root_path.iterdir():\n",
        "        if not cls_dir.is_dir():\n",
        "            continue\n",
        "        for idx, f in enumerate(cls_dir.glob('**/*')):\n",
        "            if f.suffix.lower() in exts:\n",
        "                all_files.append(str(f))\n",
        "            # Periodic batch caching\n",
        "            if len(all_files) % batch_cache == 0:\n",
        "                print(f\"Indexed {len(all_files)} files so far...\")\n",
        "\n",
        "    print(f\"Done. Found {len(all_files)} files.\")\n",
        "    return all_files\n",
        "\n",
        "def build_class_index(files, root_dir, max_classes=None):\n",
        "    \"\"\"Group images by their immediate subfolder under root_dir\"\"\"\n",
        "    root = Path(root_dir)\n",
        "    class_to_images = {}\n",
        "    for f in files:\n",
        "        rel = Path(f).resolve().relative_to(root.resolve())\n",
        "        parts = rel.parts\n",
        "        if len(parts) < 2:\n",
        "            continue  # skip images directly in root\n",
        "        cls = parts[0]  # immediate subfolder name (e.g., \"AR 101\")\n",
        "        class_to_images.setdefault(cls, []).append(f)\n",
        "    \n",
        "    # Keep only classes with >= 2 images (needed for triplets)\n",
        "    class_to_images = {k: v for k, v in class_to_images.items() if len(v) >= 2}\n",
        "    \n",
        "    # Limit number of classes if specified\n",
        "    if max_classes and len(class_to_images) > max_classes:\n",
        "        # Sort by number of images (descending) and take top classes\n",
        "        sorted_classes = sorted(class_to_images.items(), key=lambda x: len(x[1]), reverse=True)\n",
        "        class_to_images = dict(sorted_classes[:max_classes])\n",
        "\n",
        "    if len(class_to_images) == 0:\n",
        "        print(\"‚ö†Ô∏è No valid classes found! Checking dataset structure...\")\n",
        "        for dirpath, dirnames, filenames in os.walk(root_dir):\n",
        "            print(f\"{dirpath}: {len(filenames)} files\")\n",
        "            break  # only show top-level\n",
        "    else:\n",
        "        print(f\"‚úÖ Found {len(class_to_images)} classes\")\n",
        "        for cls, imgs in list(class_to_images.items())[:5]:\n",
        "            print(f\"Class: {cls}, {len(imgs)} images, sample: {imgs[0]}\")\n",
        "    \n",
        "    return class_to_images\n",
        "\n",
        "class ComicTripletDataset(Dataset):\n",
        "    def __init__(self, class_to_images, transform=None):\n",
        "        self.transform = transform\n",
        "        self.class_to_images = class_to_images\n",
        "        self.classes = list(self.class_to_images.keys())\n",
        "\n",
        "    def __len__(self):\n",
        "        # Scale with number of classes for balanced sampling\n",
        "        return max(len(self.classes) * 20, 1000)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        anchor_class = random.choice(self.classes)\n",
        "        negative_class = random.choice([c for c in self.classes if c != anchor_class])\n",
        "\n",
        "        anchor_path, positive_path = random.sample(self.class_to_images[anchor_class], 2)\n",
        "        negative_path = random.choice(self.class_to_images[negative_class])\n",
        "\n",
        "        anchor_img = Image.open(anchor_path).convert(\"RGB\")\n",
        "        positive_img = Image.open(positive_path).convert(\"RGB\")\n",
        "        negative_img = Image.open(negative_path).convert(\"RGB\")\n",
        "\n",
        "        if self.transform:\n",
        "            anchor_img = self.transform(anchor_img)\n",
        "            positive_img = self.transform(positive_img)\n",
        "            negative_img = self.transform(negative_img)\n",
        "\n",
        "        return anchor_img, positive_img, negative_img\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "DATASET_PATH = \"kaggle/input/inducks-entry-images/covers-by-storycode\"\n",
        "OUTPUT_DIR = \".\"\n",
        "MAX_CLASSES = 1000  # Adjust based on your dataset size\n",
        "EPOCHS = 3\n",
        "BATCH_SIZE = 16\n",
        "LEARNING_RATE = 1e-4\n",
        "EMBED_DIM = 512\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Dataset path and caching\n",
        "CACHE_DIR = OUTPUT_DIR\n",
        "FILE_CACHE = os.path.join(CACHE_DIR, \"file_list.pkl\")\n",
        "CLASS_CACHE = os.path.join(CACHE_DIR, \"class_to_images.pkl\")\n",
        "\n",
        "# Load or build dataset index\n",
        "if Path(CLASS_CACHE).exists():\n",
        "    with open(CLASS_CACHE, 'rb') as f:\n",
        "        class_to_images = pickle.load(f)\n",
        "    print(f\"Loaded class mapping from cache: {len(class_to_images)} classes\")\n",
        "else:\n",
        "    if Path(FILE_CACHE).exists():\n",
        "        with open(FILE_CACHE, 'rb') as f:\n",
        "            all_files = pickle.load(f)\n",
        "        print(f\"Loaded {len(all_files)} file paths from cache\")\n",
        "    else:\n",
        "        print(\"Indexing files (first run may take a while)...\")\n",
        "        all_files = get_image_files(DATASET_PATH)\n",
        "        with open(FILE_CACHE, 'wb') as f:\n",
        "            pickle.dump(all_files, f)\n",
        "        print(f\"Indexed {len(all_files)} image files and cached to {FILE_CACHE}\")\n",
        "\n",
        "    class_to_images = build_class_index(all_files, DATASET_PATH, MAX_CLASSES)\n",
        "    with open(CLASS_CACHE, 'wb') as f:\n",
        "        pickle.dump(class_to_images, f)\n",
        "    print(f\"Built class mapping: {len(class_to_images)} classes cached to {CLASS_CACHE}\")\n",
        "\n",
        "num_images = sum(len(v) for v in class_to_images.values())\n",
        "print(f\"Ready: {len(class_to_images)} classes, {num_images} images with >=2 per class.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Transforms optimized for EfficientNet\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    transforms.RandomRotation(5),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # ImageNet normalization\n",
        "])\n",
        "\n",
        "# Create dataset and dataloader\n",
        "dataset = ComicTripletDataset(class_to_images, transform=transform)\n",
        "\n",
        "NUM_WORKERS = 4 if device.type == 'cuda' else 2\n",
        "loader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True if device.type == 'cuda' else False,\n",
        "    persistent_workers=True if NUM_WORKERS > 0 else False\n",
        ")\n",
        "\n",
        "print(f\"Dataset created: {len(dataset)} samples per epoch\")\n",
        "print(f\"Batch size: {BATCH_SIZE}, Workers: {NUM_WORKERS}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create model\n",
        "model = EfficientNetEmbeddingModel(\n",
        "    model_name=\"efficientnet_b0\",\n",
        "    lr=LEARNING_RATE,\n",
        "    embed_dim=EMBED_DIM\n",
        ")\n",
        "\n",
        "# Setup trainer\n",
        "num_gpus = torch.cuda.device_count()\n",
        "\n",
        "if num_gpus > 0:\n",
        "    accelerator = \"gpu\"\n",
        "    devices = min(num_gpus, 2)  # Use up to 2 GPUs\n",
        "    strategy = \"auto\"\n",
        "    precision = 16  # Mixed precision for efficiency\n",
        "else:\n",
        "    accelerator = \"cpu\"\n",
        "    devices = 1\n",
        "    strategy = \"auto\"\n",
        "    precision = 32\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs=EPOCHS,\n",
        "    accelerator=accelerator,\n",
        "    devices=devices,\n",
        "    strategy=strategy,\n",
        "    precision=precision,\n",
        "    log_every_n_steps=10,\n",
        "    val_check_interval=0.5,  # Validate every 50% of epoch\n",
        "    enable_progress_bar=True,\n",
        "    enable_model_summary=True\n",
        ")\n",
        "\n",
        "print(f\"Trainer configured: {accelerator}, {devices} devices, precision={precision}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model\n",
        "print(f\"\\nüöÄ Starting training for {EPOCHS} epochs...\")\n",
        "trainer.fit(model, loader)\n",
        "\n",
        "# Save the model\n",
        "model_path = os.path.join(OUTPUT_DIR, \"efficientnet_b0_comic_embedding.pt\")\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(f\"‚úÖ Model saved to {model_path}\")\n",
        "\n",
        "# Save checkpoint\n",
        "checkpoint_path = os.path.join(OUTPUT_DIR, f\"efficientnet_b0_epoch{EPOCHS}.ckpt\")\n",
        "trainer.save_checkpoint(checkpoint_path)\n",
        "print(f\"‚úÖ Checkpoint saved to {checkpoint_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training loss\n",
        "if model.training_losses:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(model.training_losses, label=\"Training Loss\", alpha=0.7)\n",
        "    plt.xlabel(\"Batch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"EfficientNet-B0 Training Loss\")\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    plot_path = os.path.join(OUTPUT_DIR, \"training_loss.png\")\n",
        "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(f\"‚úÖ Training loss plot saved to {plot_path}\")\n",
        "    \n",
        "    print(f\"\\nüìä Final training loss: {model.training_losses[-1]:.4f}\")\n",
        "    print(f\"üìà Loss improvement: {model.training_losses[0]:.4f} ‚Üí {model.training_losses[-1]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export to ONNX\n",
        "try:\n",
        "    model.eval()\n",
        "    dummy_input = torch.randn(1, 3, 224, 224)\n",
        "    \n",
        "    onnx_path = os.path.join(OUTPUT_DIR, \"efficientnet_b0_comic_embedding.onnx\")\n",
        "    torch.onnx.export(\n",
        "        model,\n",
        "        dummy_input,\n",
        "        onnx_path,\n",
        "        input_names=[\"input\"],\n",
        "        output_names=[\"embedding\"],\n",
        "        dynamic_axes={\"input\": {0: \"batch\"}, \"embedding\": {0: \"batch\"}},\n",
        "        opset_version=17,\n",
        "        export_params=True\n",
        "    )\n",
        "    \n",
        "    onnx_size_mb = os.path.getsize(onnx_path) / (1024 * 1024)\n",
        "    print(f\"‚úÖ ONNX model exported to {onnx_path} ({onnx_size_mb:.1f} MB)\")\n",
        "    \n",
        "    # Compare with original ViT-Base size\n",
        "    vit_size_mb = 331.1\n",
        "    size_reduction = (1 - onnx_size_mb / vit_size_mb) * 100\n",
        "    print(f\"üìä Size reduction vs ViT-Base: {size_reduction:.1f}% ({vit_size_mb:.1f}MB ‚Üí {onnx_size_mb:.1f}MB)\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è ONNX export failed: {e}\")\n",
        "\n",
        "print(\"\\nüéâ Training completed successfully!\")\n",
        "print(f\"üìÅ All outputs saved to: {OUTPUT_DIR}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
