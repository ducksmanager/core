{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# EfficientNet-B0 Fine-tuning for Comic Reverse Image Search\n",
        "\n",
        "This notebook fine-tunes EfficientNet-B0 for comic reverse image search using triplet loss.\n",
        "\n",
        "**Key advantages of EfficientNet-B0:**\n",
        "- **Small size**: ~18MB ONNX (vs 331MB for ViT-Base)\n",
        "- **High quality**: 0.136 similarity separation in our tests\n",
        "- **Fast inference**: ~264ms per batch\n",
        "- **Proven architecture**: EfficientNet is well-established\n",
        "\n",
        "**Training approach:**\n",
        "- Uses triplet loss for similarity learning\n",
        "- Pre-trained on ImageNet for better initialization\n",
        "- Enhanced data augmentation for robustness (blur, rotation, partial frames)\n",
        "- Regularization techniques to prevent overfitting\n",
        "- Mixed precision training for efficiency\n",
        "\n",
        "**Improvements for robustness:**\n",
        "- **Blurred images**: Random Gaussian blur augmentation\n",
        "- **Rotated images**: Random affine transformations with up to 30¬∞ rotation\n",
        "- **Partial frame images**: Random resized crops to simulate incomplete views\n",
        "- **Regularization**: Dropout, weight decay, learning rate scheduling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install torch torchvision pytorch-lightning timm matplotlib tqdm --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import TripletMarginLoss\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import functional as TF\n",
        "from PIL import Image, ImageFilter\n",
        "import pytorch_lightning as pl\n",
        "import timm\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "pl.seed_everything(42)\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# =============================================\n",
        "# Custom augmentation transforms for robustness\n",
        "# =============================================\n",
        "class RandomGaussianBlur:\n",
        "    \"\"\"Apply random Gaussian blur to simulate slightly blurred images\"\"\"\n",
        "    def __init__(self, p=0.5, radius_range=(0, 2.5)):\n",
        "        self.p = p\n",
        "        self.radius_range = radius_range\n",
        "    \n",
        "    def __call__(self, img):\n",
        "        if random.random() < self.p:\n",
        "            radius = random.uniform(*self.radius_range)\n",
        "            if radius > 0:\n",
        "                img = img.filter(ImageFilter.GaussianBlur(radius=radius))\n",
        "        return img\n",
        "\n",
        "class RandomPerspective:\n",
        "    \"\"\"Apply random perspective transformation\"\"\"\n",
        "    def __init__(self, p=0.3, distortion_scale=0.2):\n",
        "        self.p = p\n",
        "        self.distortion_scale = distortion_scale\n",
        "    \n",
        "    def __call__(self, img):\n",
        "        if random.random() < self.p:\n",
        "            return TF.perspective(img, \n",
        "                                 startpoints=[(0, 0), (img.width, 0), (img.width, img.height), (0, img.height)],\n",
        "                                 endpoints=self._get_random_endpoints(img),\n",
        "                                 interpolation=TF.InterpolationMode.BILINEAR)\n",
        "        return img\n",
        "    \n",
        "    def _get_random_endpoints(self, img):\n",
        "        w, h = img.width, img.height\n",
        "        d = int(min(w, h) * self.distortion_scale)\n",
        "        return [\n",
        "            (random.randint(-d, d), random.randint(-d, d)),\n",
        "            (w + random.randint(-d, d), random.randint(-d, d)),\n",
        "            (w + random.randint(-d, d), h + random.randint(-d, d)),\n",
        "            (random.randint(-d, d), h + random.randint(-d, d))\n",
        "        ]\n",
        "\n",
        "class RandomAffine:\n",
        "    \"\"\"Apply random affine transformation (rotation, translation, scale, shear)\"\"\"\n",
        "    def __init__(self, degrees=30, translate=(0.1, 0.1), scale=(0.8, 1.2), shear=10, p=0.7):\n",
        "        self.degrees = degrees\n",
        "        self.translate = translate\n",
        "        self.scale = scale\n",
        "        self.shear = shear\n",
        "        self.p = p\n",
        "    \n",
        "    def __call__(self, img):\n",
        "        if random.random() < self.p:\n",
        "            angle = random.uniform(-self.degrees, self.degrees)\n",
        "            translate = (random.uniform(-self.translate[0], self.translate[0]) * img.width,\n",
        "                        random.uniform(-self.translate[1], self.translate[1]) * img.height)\n",
        "            scale = random.uniform(*self.scale)\n",
        "            shear = random.uniform(-self.shear, self.shear)\n",
        "            return TF.affine(img, angle=angle, translate=translate, scale=scale, shear=shear,\n",
        "                           interpolation=TF.InterpolationMode.BILINEAR)\n",
        "        return img\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EfficientNetEmbeddingModel(pl.LightningModule):\n",
        "    def __init__(self, model_name=\"efficientnet_b0\", lr=1e-4, embed_dim=512, dropout=0.3, weight_decay=0.01):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        \n",
        "        # Load EfficientNet-B0 backbone\n",
        "        self.backbone = timm.create_model(model_name, pretrained=True, num_classes=0)  # Remove classifier\n",
        "        self.feature_dim = self.backbone.num_features  # 1280 for EfficientNet-B0\n",
        "        \n",
        "        # Two-layer projection head with dropout for regularization\n",
        "        self.fc1 = nn.Linear(self.feature_dim, self.feature_dim)\n",
        "        self.bn = nn.BatchNorm1d(self.feature_dim)  # Batch normalization for stability\n",
        "        self.dropout = nn.Dropout(dropout)  # Dropout to prevent overfitting\n",
        "        self.fc2 = nn.Linear(self.feature_dim, embed_dim)\n",
        "        \n",
        "        # Loss function\n",
        "        self.loss_fn = TripletMarginLoss(margin=0.2)\n",
        "        self.training_losses = []\n",
        "        \n",
        "        print(f\"EfficientNet-B0: feature_dim={self.feature_dim}, embed_dim={embed_dim}, dropout={dropout}\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)\n",
        "        # Two-layer projection head with dropout for regularization\n",
        "        x = self.fc1(features)\n",
        "        x = self.bn(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        embeddings = self.fc2(x)\n",
        "        return F.normalize(embeddings, p=2, dim=1)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        anchor, positive, negative = batch\n",
        "        emb_a = self(anchor)\n",
        "        emb_p = self(positive)\n",
        "        emb_n = self(negative)\n",
        "        loss = self.loss_fn(emb_a, emb_p, emb_n)\n",
        "        self.training_losses.append(loss.item())\n",
        "        self.log(\"train_loss\", loss, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            self.parameters(), \n",
        "            lr=self.hparams.lr,\n",
        "            weight_decay=self.hparams.weight_decay  # L2 regularization\n",
        "        )\n",
        "        # Learning rate scheduler to reduce overfitting\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "            optimizer, \n",
        "            T_max=1000,  # Adjust based on your training steps\n",
        "            eta_min=1e-7\n",
        "        )\n",
        "        return {\n",
        "            \"optimizer\": optimizer,\n",
        "            \"lr_scheduler\": {\n",
        "                \"scheduler\": scheduler,\n",
        "                \"interval\": \"step\"\n",
        "            }\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset loading functions (same as original)\n",
        "def get_image_files(root_dir, batch_cache=10000):\n",
        "    \"\"\"Fast, cached file indexing\"\"\"\n",
        "    print(f\"Scanning for image files in {root_dir} ...\")\n",
        "    exts = {'.jpg', '.jpeg', '.png'}\n",
        "    all_files = []\n",
        "\n",
        "    root_path = Path(root_dir)\n",
        "    for cls_dir in root_path.iterdir():\n",
        "        if not cls_dir.is_dir():\n",
        "            continue\n",
        "        for idx, f in enumerate(cls_dir.glob('**/*')):\n",
        "            if f.suffix.lower() in exts:\n",
        "                all_files.append(str(f))\n",
        "            # Periodic batch caching\n",
        "            if len(all_files) % batch_cache == 0:\n",
        "                print(f\"Indexed {len(all_files)} files so far...\")\n",
        "\n",
        "    print(f\"Done. Found {len(all_files)} files.\")\n",
        "    return all_files\n",
        "\n",
        "def build_class_index(files, root_dir, max_classes=None):\n",
        "    \"\"\"Group images by their immediate subfolder under root_dir\"\"\"\n",
        "    root = Path(root_dir)\n",
        "    class_to_images = {}\n",
        "    for f in files:\n",
        "        rel = Path(f).resolve().relative_to(root.resolve())\n",
        "        parts = rel.parts\n",
        "        if len(parts) < 2:\n",
        "            continue  # skip images directly in root\n",
        "        cls = parts[0]  # immediate subfolder name (e.g., \"AR 101\")\n",
        "        class_to_images.setdefault(cls, []).append(f)\n",
        "    \n",
        "    # Keep only classes with >= 2 images (needed for triplets)\n",
        "    class_to_images = {k: v for k, v in class_to_images.items() if len(v) >= 2}\n",
        "    \n",
        "    # Limit number of classes if specified\n",
        "    if max_classes and len(class_to_images) > max_classes:\n",
        "        # Sort by number of images (descending) and take top classes\n",
        "        sorted_classes = sorted(class_to_images.items(), key=lambda x: len(x[1]), reverse=True)\n",
        "        class_to_images = dict(sorted_classes[:max_classes])\n",
        "\n",
        "    if len(class_to_images) == 0:\n",
        "        print(\"‚ö†Ô∏è No valid classes found! Checking dataset structure...\")\n",
        "        for dirpath, dirnames, filenames in os.walk(root_dir):\n",
        "            print(f\"{dirpath}: {len(filenames)} files\")\n",
        "            break  # only show top-level\n",
        "    else:\n",
        "        print(f\"‚úÖ Found {len(class_to_images)} classes\")\n",
        "        for cls, imgs in list(class_to_images.items())[:5]:\n",
        "            print(f\"Class: {cls}, {len(imgs)} images, sample: {imgs[0]}\")\n",
        "    \n",
        "    return class_to_images\n",
        "\n",
        "class ComicTripletDataset(Dataset):\n",
        "    def __init__(self, class_to_images, transform=None, samples_per_class=20):\n",
        "        self.transform = transform\n",
        "        self.class_to_images = class_to_images\n",
        "        self.classes = list(self.class_to_images.keys())\n",
        "        self.samples_per_class = samples_per_class\n",
        "\n",
        "    def __len__(self):\n",
        "        # Increase samples per class for better coverage and diversity\n",
        "        # More samples = more diverse augmentations seen during training\n",
        "        return max(len(self.classes) * self.samples_per_class, 1000)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        anchor_class = random.choice(self.classes)\n",
        "        negative_class = random.choice([c for c in self.classes if c != anchor_class])\n",
        "\n",
        "        anchor_path, positive_path = random.sample(self.class_to_images[anchor_class], 2)\n",
        "        negative_path = random.choice(self.class_to_images[negative_class])\n",
        "\n",
        "        anchor_img = Image.open(anchor_path).convert(\"RGB\")\n",
        "        positive_img = Image.open(positive_path).convert(\"RGB\")\n",
        "        negative_img = Image.open(negative_path).convert(\"RGB\")\n",
        "\n",
        "        if self.transform:\n",
        "            anchor_img = self.transform(anchor_img)\n",
        "            positive_img = self.transform(positive_img)\n",
        "            negative_img = self.transform(negative_img)\n",
        "\n",
        "        return anchor_img, positive_img, negative_img\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "# Try to auto-detect the dataset path in Kaggle (using absolute paths only)\n",
        "POSSIBLE_PATHS = [\n",
        "    \"/kaggle/input/inducks-entry-images/covers-by-storycode\",\n",
        "    \"/kaggle/input/inducks-entry-images/Inducks entry images/covers-by-storycode\",  # With spaces\n",
        "]\n",
        "\n",
        "DATASET_PATH = None\n",
        "for path in POSSIBLE_PATHS:\n",
        "    abs_path = Path(path).resolve()\n",
        "    if abs_path.exists():\n",
        "        DATASET_PATH = str(abs_path)\n",
        "        print(f\"‚úÖ Found dataset at: {DATASET_PATH}\")\n",
        "        break\n",
        "\n",
        "if DATASET_PATH is None:\n",
        "    print(\"‚ö†Ô∏è Dataset path not found. Checking available directories...\")\n",
        "    # Check if /kaggle/input exists and list its contents\n",
        "    if Path(\"/kaggle/input\").exists():\n",
        "        print(\"\\nAvailable directories in /kaggle/input:\")\n",
        "        for item in Path(\"/kaggle/input\").iterdir():\n",
        "            print(f\"  - {item}\")\n",
        "            if item.is_dir():\n",
        "                # Check subdirectories\n",
        "                try:\n",
        "                    for subitem in item.iterdir():\n",
        "                        print(f\"    - {subitem}\")\n",
        "                except:\n",
        "                    pass\n",
        "    raise FileNotFoundError(\n",
        "        f\"Could not find dataset. Please set DATASET_PATH manually to an absolute path.\\n\"\n",
        "        f\"Expected structure: /kaggle/input/<dataset-name>/covers-by-storycode/<storycode-folders>/<images>\"\n",
        "    )\n",
        "\n",
        "# Ensure DATASET_PATH is absolute\n",
        "DATASET_PATH = str(Path(DATASET_PATH).resolve())\n",
        "print(f\"üìÅ Using absolute dataset path: {DATASET_PATH}\")\n",
        "\n",
        "# OUTPUT_DIR should also be absolute\n",
        "OUTPUT_DIR = \"/kaggle/working\" if Path(\"/kaggle/working\").exists() else str(Path(\".\").resolve())\n",
        "OUTPUT_DIR = str(Path(OUTPUT_DIR).resolve())\n",
        "print(f\"üìÅ Using absolute output directory: {OUTPUT_DIR}\")\n",
        "\n",
        "# Training configuration\n",
        "EPOCHS = 3\n",
        "BATCH_SIZE = 16\n",
        "LEARNING_RATE = 1e-4\n",
        "EMBED_DIM = 512\n",
        "SAMPLES_PER_CLASS = 20  # Samples per class per epoch\n",
        "\n",
        "# Limit number of classes for training (to keep training time reasonable)\n",
        "# Strategy: Use top classes by number of images (these have most examples = better learning)\n",
        "# The model learns a GENERAL similarity function, so it can detect ANY storycode at inference,\n",
        "# but training on diverse classes helps generalization\n",
        "MAX_CLASSES_FOR_TRAINING = 5000  # Adjust based on available compute time\n",
        "# With 5000 classes √ó 20 samples √ó 3 epochs = 300k samples total (~18k batches per epoch)\n",
        "# This should take ~2-3 hours per epoch instead of 20 hours\n",
        "#\n",
        "# Note: This model uses triplet loss for similarity learning, NOT classification.\n",
        "# At inference, it compares query embeddings against ALL storycodes in the database,\n",
        "# so it CAN detect storycodes not seen during training. Quality may vary.\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Dataset path and caching (using absolute paths)\n",
        "CACHE_DIR = OUTPUT_DIR\n",
        "FILE_CACHE = str(Path(CACHE_DIR) / \"file_list.pkl\")\n",
        "CLASS_CACHE = str(Path(CACHE_DIR) / \"class_to_images.pkl\")\n",
        "\n",
        "# Load or build dataset index\n",
        "if Path(CLASS_CACHE).exists():\n",
        "    with open(CLASS_CACHE, 'rb') as f:\n",
        "        all_class_to_images = pickle.load(f)\n",
        "    print(f\"Loaded class mapping from cache: {len(all_class_to_images)} total classes\")\n",
        "    \n",
        "    # Apply training limit if needed\n",
        "    if len(all_class_to_images) > MAX_CLASSES_FOR_TRAINING:\n",
        "        print(f\"\\n‚ö†Ô∏è Found {len(all_class_to_images)} classes, limiting to top {MAX_CLASSES_FOR_TRAINING} for training\")\n",
        "        sorted_classes = sorted(all_class_to_images.items(), key=lambda x: len(x[1]), reverse=True)\n",
        "        class_to_images = dict(sorted_classes[:MAX_CLASSES_FOR_TRAINING])\n",
        "        print(f\"‚úÖ Using top {len(class_to_images)} classes (by number of images)\")\n",
        "        print(f\"   Note: Model learns general similarity, so it can detect ALL storycodes at inference\")\n",
        "    else:\n",
        "        class_to_images = all_class_to_images\n",
        "        print(f\"‚úÖ Using all {len(class_to_images)} classes\")\n",
        "else:\n",
        "    if Path(FILE_CACHE).exists():\n",
        "        with open(FILE_CACHE, 'rb') as f:\n",
        "            all_files = pickle.load(f)\n",
        "        print(f\"Loaded {len(all_files)} file paths from cache\")\n",
        "    else:\n",
        "        print(\"Indexing files (first run may take a while)...\")\n",
        "        all_files = get_image_files(DATASET_PATH)\n",
        "        with open(FILE_CACHE, 'wb') as f:\n",
        "            pickle.dump(all_files, f)\n",
        "        print(f\"Indexed {len(all_files)} image files and cached to {FILE_CACHE}\")\n",
        "\n",
        "    # Build class index - first get all classes to see what we have\n",
        "    all_class_to_images = build_class_index(all_files, DATASET_PATH, max_classes=None)\n",
        "    with open(CLASS_CACHE, 'wb') as f:\n",
        "        pickle.dump(all_class_to_images, f)\n",
        "    print(f\"Built class mapping: {len(all_class_to_images)} total classes found\")\n",
        "    \n",
        "    # Now limit to top classes for training\n",
        "    if len(all_class_to_images) > MAX_CLASSES_FOR_TRAINING:\n",
        "        print(f\"\\n‚ö†Ô∏è Found {len(all_class_to_images)} classes, limiting to top {MAX_CLASSES_FOR_TRAINING} for training\")\n",
        "        # Sort by number of images (descending) and take top classes\n",
        "        # This ensures we train on classes with most examples (better for learning)\n",
        "        sorted_classes = sorted(all_class_to_images.items(), key=lambda x: len(x[1]), reverse=True)\n",
        "        class_to_images = dict(sorted_classes[:MAX_CLASSES_FOR_TRAINING])\n",
        "        print(f\"‚úÖ Using top {len(class_to_images)} classes (by number of images)\")\n",
        "        print(f\"   Note: Model learns general similarity, so it can detect ALL storycodes at inference\")\n",
        "        print(f\"   Training on diverse classes helps generalization to unseen storycodes\")\n",
        "    else:\n",
        "        class_to_images = all_class_to_images\n",
        "        print(f\"‚úÖ Using all {len(class_to_images)} classes\")\n",
        "\n",
        "num_images = sum(len(v) for v in class_to_images.values())\n",
        "avg_images_per_class = num_images / len(class_to_images) if len(class_to_images) > 0 else 0\n",
        "print(f\"\\nüìä Training Dataset Summary:\")\n",
        "print(f\"  - Classes: {len(class_to_images)}\")\n",
        "print(f\"  - Total images: {num_images}\")\n",
        "print(f\"  - Avg images per class: {avg_images_per_class:.1f}\")\n",
        "print(f\"  - Samples per epoch: {len(class_to_images) * SAMPLES_PER_CLASS}\")\n",
        "print(f\"  - Estimated batches per epoch: {(len(class_to_images) * SAMPLES_PER_CLASS) // BATCH_SIZE}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enhanced transforms for robustness (blur, rotation, partial frames)\n",
        "# Aggressive data augmentation to handle:\n",
        "# - Blurred images (RandomGaussianBlur)\n",
        "# - Rotated images (RandomAffine with up to 30¬∞ rotation)\n",
        "# - Partial frame images (RandomResizedCrop with scale variation)\n",
        "# - Perspective distortions (RandomPerspective)\n",
        "transform = transforms.Compose([\n",
        "    # First resize to larger size to allow for cropping\n",
        "    transforms.Resize((256, 256)),\n",
        "    \n",
        "    # Random crop to simulate partial frame images (70-100% of image)\n",
        "    # This helps model learn to recognize images even when not fully visible\n",
        "    transforms.RandomResizedCrop(\n",
        "        size=224,\n",
        "        scale=(0.7, 1.0),  # Can crop up to 30% of image\n",
        "        ratio=(0.8, 1.25)  # Allow some aspect ratio variation\n",
        "    ),\n",
        "    \n",
        "    # Color augmentations (more aggressive)\n",
        "    transforms.ColorJitter(\n",
        "        brightness=0.3,  # Increased from 0.2\n",
        "        contrast=0.3,    # Increased from 0.2\n",
        "        saturation=0.3,  # Increased from 0.2\n",
        "        hue=0.1          # Added hue variation\n",
        "    ),\n",
        "    \n",
        "    # Random blur to handle slightly blurred images\n",
        "    RandomGaussianBlur(p=0.5, radius_range=(0, 2.5)),\n",
        "    \n",
        "    # Affine transformations: rotation (up to 30¬∞), translation, scale, shear\n",
        "    RandomAffine(degrees=30, translate=(0.15, 0.15), scale=(0.75, 1.25), shear=15, p=0.7),\n",
        "    \n",
        "    # Perspective transformation for geometric robustness\n",
        "    RandomPerspective(p=0.3, distortion_scale=0.2),\n",
        "    \n",
        "    # Horizontal flip\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    \n",
        "    # Convert to tensor and normalize (ImageNet normalization for EfficientNet)\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    \n",
        "    # Random erasing (cutout) for additional regularization\n",
        "    transforms.RandomErasing(p=0.2, scale=(0.02, 0.1), ratio=(0.3, 3.3)),\n",
        "])\n",
        "\n",
        "# Create dataset and dataloader\n",
        "dataset = ComicTripletDataset(class_to_images, transform=transform, samples_per_class=SAMPLES_PER_CLASS)\n",
        "\n",
        "NUM_WORKERS = 4 if device.type == 'cuda' else 2\n",
        "loader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True if device.type == 'cuda' else False,\n",
        "    persistent_workers=True if NUM_WORKERS > 0 else False\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Dataset created: {len(dataset)} samples per epoch\")\n",
        "print(f\"   Batch size: {BATCH_SIZE}, Workers: {NUM_WORKERS}\")\n",
        "print(f\"   Actual batches per epoch: {len(loader)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create model with regularization parameters\n",
        "# dropout=0.3 and weight_decay=0.01 help prevent overfitting\n",
        "model = EfficientNetEmbeddingModel(\n",
        "    model_name=\"efficientnet_b0\",\n",
        "    lr=LEARNING_RATE,\n",
        "    embed_dim=EMBED_DIM,\n",
        "    dropout=0.3,        # Dropout for regularization\n",
        "    weight_decay=0.01  # L2 weight decay\n",
        ")\n",
        "\n",
        "# Setup trainer\n",
        "num_gpus = torch.cuda.device_count()\n",
        "\n",
        "# Use single GPU for stability (multi-GPU can cause hangs in notebooks)\n",
        "if num_gpus > 0:\n",
        "    accelerator = \"gpu\"\n",
        "    devices = 1  # Use single GPU to avoid DDP issues in notebooks\n",
        "    strategy = \"auto\"\n",
        "    precision = \"16-mixed\"  # Fixed: use \"16-mixed\" instead of 16 for mixed precision\n",
        "else:\n",
        "    accelerator = \"cpu\"\n",
        "    devices = 1\n",
        "    strategy = \"auto\"\n",
        "    precision = \"32-true\"\n",
        "\n",
        "# Custom callback to print progress\n",
        "class ProgressCallback(pl.Callback):\n",
        "    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n",
        "        if batch_idx % 100 == 0:  # Print every 100 batches\n",
        "            current_epoch = trainer.current_epoch + 1\n",
        "            total_batches = len(trainer.train_dataloader)\n",
        "            progress = (batch_idx + 1) / total_batches * 100\n",
        "            loss = outputs['loss'].item() if isinstance(outputs, dict) else outputs.item()\n",
        "            print(f\"Epoch {current_epoch}/{EPOCHS}, Batch {batch_idx+1}/{total_batches} ({progress:.1f}%), Loss: {loss:.4f}\")\n",
        "\n",
        "# Print dataset info before training\n",
        "print(f\"\\nüìä Dataset Info:\")\n",
        "print(f\"  - Total samples per epoch: {len(dataset)}\")\n",
        "print(f\"  - Batch size: {BATCH_SIZE}\")\n",
        "print(f\"  - Batches per epoch: {len(loader)}\")\n",
        "estimated_time_per_epoch = len(loader) * 0.3  # More realistic estimate: ~0.3s per batch\n",
        "print(f\"  - Estimated time per epoch: ~{estimated_time_per_epoch/60:.1f} minutes ({estimated_time_per_epoch:.0f} seconds)\")\n",
        "\n",
        "# Test data loading first\n",
        "print(f\"\\nüß™ Testing data loading...\")\n",
        "try:\n",
        "    test_batch = next(iter(loader))\n",
        "    print(f\"‚úÖ Data loading works! Batch shape: {[x.shape for x in test_batch]}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Data loading failed: {e}\")\n",
        "    raise\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs=EPOCHS,\n",
        "    accelerator=accelerator,\n",
        "    devices=devices,\n",
        "    strategy=strategy,\n",
        "    precision=precision,\n",
        "    log_every_n_steps=100,  # Log every 100 steps\n",
        "    enable_progress_bar=True,\n",
        "    enable_model_summary=True,\n",
        "    # Add gradient clipping to prevent exploding gradients\n",
        "    gradient_clip_val=1.0,\n",
        "    # Add callback for progress monitoring\n",
        "    callbacks=[ProgressCallback()],\n",
        "    # Remove val_check_interval since we don't have a validation set\n",
        "    # This was causing issues - validation check with no validation set can hang\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Trainer configured: {accelerator}, {devices} device(s), precision={precision}\")\n",
        "print(f\"   Will train for {EPOCHS} epochs (~{len(loader) * EPOCHS} total batches)\")\n",
        "print(f\"   Estimated total time: ~{estimated_time_per_epoch * EPOCHS / 60:.1f} minutes\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model\n",
        "print(f\"\\nüöÄ Starting training for {EPOCHS} epochs...\")\n",
        "print(f\"   Dataset: {len(dataset)} samples, {len(loader)} batches per epoch\")\n",
        "print(f\"   Progress will be printed every 100 batches\\n\")\n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "try:\n",
        "    trainer.fit(model, loader)\n",
        "    elapsed = time.time() - start_time\n",
        "    print(f\"\\n‚úÖ Training completed!\")\n",
        "    print(f\"   Total time: {elapsed/60:.1f} minutes ({elapsed:.1f} seconds)\")\n",
        "    print(f\"   Average time per epoch: {elapsed/EPOCHS/60:.1f} minutes\")\n",
        "except KeyboardInterrupt:\n",
        "    elapsed = time.time() - start_time\n",
        "    print(f\"\\n‚ö†Ô∏è Training interrupted by user after {elapsed/60:.1f} minutes\")\n",
        "    raise\n",
        "except Exception as e:\n",
        "    elapsed = time.time() - start_time\n",
        "    print(f\"\\n‚ùå Training failed after {elapsed/60:.1f} minutes\")\n",
        "    print(f\"   Error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    raise\n",
        "\n",
        "# Save the model (using absolute paths)\n",
        "model_path = str(Path(OUTPUT_DIR) / \"efficientnet_b0_comic_embedding.pt\")\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(f\"‚úÖ Model saved to {model_path}\")\n",
        "\n",
        "# Save checkpoint\n",
        "checkpoint_path = str(Path(OUTPUT_DIR) / f\"efficientnet_b0_epoch{EPOCHS}.ckpt\")\n",
        "trainer.save_checkpoint(checkpoint_path)\n",
        "print(f\"‚úÖ Checkpoint saved to {checkpoint_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training loss with analysis\n",
        "if model.training_losses:\n",
        "    import numpy as np\n",
        "    \n",
        "    losses = np.array(model.training_losses)\n",
        "    \n",
        "    # Calculate moving average for smoother visualization\n",
        "    window_size = 100\n",
        "    if len(losses) > window_size:\n",
        "        moving_avg = np.convolve(losses, np.ones(window_size)/window_size, mode='valid')\n",
        "        moving_avg_x = np.arange(window_size//2, len(losses) - window_size//2)\n",
        "    else:\n",
        "        moving_avg = losses\n",
        "        moving_avg_x = np.arange(len(losses))\n",
        "    \n",
        "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
        "    \n",
        "    # Plot 1: Full loss with moving average\n",
        "    ax1.plot(losses, label=\"Training Loss\", alpha=0.3, linewidth=0.5)\n",
        "    if len(losses) > window_size:\n",
        "        ax1.plot(moving_avg_x, moving_avg, label=f\"Moving Average (window={window_size})\", \n",
        "                linewidth=2, color='red')\n",
        "    ax1.set_xlabel(\"Batch\")\n",
        "    ax1.set_ylabel(\"Loss\")\n",
        "    ax1.set_title(\"EfficientNet-B0 Training Loss (Full)\")\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 2: Last 25% of training to see final convergence\n",
        "    last_quarter_start = len(losses) // 4 * 3\n",
        "    ax2.plot(range(last_quarter_start, len(losses)), losses[last_quarter_start:], \n",
        "            label=\"Training Loss (Last 25%)\", alpha=0.5, linewidth=0.5)\n",
        "    if len(losses) > window_size:\n",
        "        last_quarter_mask = moving_avg_x >= last_quarter_start\n",
        "        ax2.plot(moving_avg_x[last_quarter_mask], moving_avg[last_quarter_mask], \n",
        "                label=f\"Moving Average\", linewidth=2, color='red')\n",
        "    ax2.set_xlabel(\"Batch\")\n",
        "    ax2.set_ylabel(\"Loss\")\n",
        "    ax2.set_title(\"Training Loss - Final 25% (Convergence Check)\")\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plot_path = str(Path(OUTPUT_DIR) / \"training_loss.png\")\n",
        "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(f\"‚úÖ Training loss plot saved to {plot_path}\")\n",
        "    \n",
        "    # Calculate statistics\n",
        "    initial_loss = np.mean(losses[:100])\n",
        "    final_loss = np.mean(losses[-100:])\n",
        "    min_loss = np.min(losses)\n",
        "    max_loss = np.max(losses)\n",
        "    std_loss = np.std(losses)\n",
        "    final_std = np.std(losses[-1000:]) if len(losses) > 1000 else std_loss\n",
        "    \n",
        "    print(f\"\\nüìä Training Loss Statistics:\")\n",
        "    print(f\"  - Initial loss (first 100 batches): {initial_loss:.4f}\")\n",
        "    print(f\"  - Final loss (last 100 batches): {final_loss:.4f}\")\n",
        "    print(f\"  - Overall improvement: {initial_loss:.4f} ‚Üí {final_loss:.4f} ({((initial_loss-final_loss)/initial_loss*100):.1f}% reduction)\")\n",
        "    print(f\"  - Minimum loss: {min_loss:.4f}\")\n",
        "    print(f\"  - Maximum loss: {max_loss:.4f}\")\n",
        "    print(f\"  - Overall std dev: {std_loss:.4f}\")\n",
        "    print(f\"  - Final std dev (last 1000 batches): {final_std:.4f}\")\n",
        "    \n",
        "    # Assessment\n",
        "    print(f\"\\nüìà Assessment:\")\n",
        "    if final_loss < 0.1:\n",
        "        print(f\"  ‚úÖ Final loss is low (< 0.1), indicating good learning\")\n",
        "    elif final_loss < 0.15:\n",
        "        print(f\"  ‚ö†Ô∏è Final loss is moderate (0.1-0.15), model may benefit from more training\")\n",
        "    else:\n",
        "        print(f\"  ‚ö†Ô∏è Final loss is high (> 0.15), consider adjusting hyperparameters\")\n",
        "    \n",
        "    if final_std < 0.05:\n",
        "        print(f\"  ‚úÖ Low variance in final loss, stable training\")\n",
        "    elif final_std < 0.1:\n",
        "        print(f\"  ‚ö†Ô∏è Moderate variance - acceptable for triplet loss with random sampling\")\n",
        "    else:\n",
        "        print(f\"  ‚ö†Ô∏è High variance - consider reducing learning rate or using smoother sampling\")\n",
        "    \n",
        "    if (initial_loss - final_loss) / initial_loss > 0.5:\n",
        "        print(f\"  ‚úÖ Significant improvement (>50% reduction), model learned effectively\")\n",
        "    else:\n",
        "        print(f\"  ‚ö†Ô∏è Limited improvement, may need more training or hyperparameter tuning\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export to ONNX\n",
        "try:\n",
        "    model.eval()\n",
        "    dummy_input = torch.randn(1, 3, 224, 224)\n",
        "    \n",
        "    onnx_path = str(Path(OUTPUT_DIR) / \"efficientnet_b0_comic_embedding.onnx\")\n",
        "    torch.onnx.export(\n",
        "        model,\n",
        "        dummy_input,\n",
        "        onnx_path,\n",
        "        input_names=[\"input\"],\n",
        "        output_names=[\"embedding\"],\n",
        "        dynamic_axes={\"input\": {0: \"batch\"}, \"embedding\": {0: \"batch\"}},\n",
        "        opset_version=17,\n",
        "        export_params=True\n",
        "    )\n",
        "    \n",
        "    onnx_size_mb = os.path.getsize(onnx_path) / (1024 * 1024)\n",
        "    print(f\"‚úÖ ONNX model exported to {onnx_path} ({onnx_size_mb:.1f} MB)\")\n",
        "    \n",
        "    # Compare with original ViT-Base size\n",
        "    vit_size_mb = 331.1\n",
        "    size_reduction = (1 - onnx_size_mb / vit_size_mb) * 100\n",
        "    print(f\"üìä Size reduction vs ViT-Base: {size_reduction:.1f}% ({vit_size_mb:.1f}MB ‚Üí {onnx_size_mb:.1f}MB)\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è ONNX export failed: {e}\")\n",
        "\n",
        "print(\"\\nüéâ Training completed successfully!\")\n",
        "print(f\"üìÅ All outputs saved to: {OUTPUT_DIR}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
