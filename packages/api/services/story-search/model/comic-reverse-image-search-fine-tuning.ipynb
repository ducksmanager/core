{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12773521,"sourceType":"datasetVersion","datasetId":8075277}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch torchvision pytorch-lightning pytorch-metric-learning transformers --quiet\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T16:10:01.097389Z","iopub.execute_input":"2025-08-16T16:10:01.097659Z","iopub.status.idle":"2025-08-16T16:10:04.657937Z","shell.execute_reply.started":"2025-08-16T16:10:01.097637Z","shell.execute_reply":"2025-08-16T16:10:04.656763Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Comic Reverse Image Search — Large Dataset Fine-Tuning\n\nThis notebook fine-tunes a ViT-based embedding model with triplet loss for reverse image search on comic images.\n\n**Optimized for large datasets (~500k files):**\n- Single-pass, **cached file indexing** (avoids repeated, slow recursive scans)\n- **Recursive subdirectory** support\n- **Single GPU by default**; optional **DDP** toggle\n- Mixed precision (FP16) for speed\n- Live loss in progress bar + **loss plot** after training\n- **ONNX export** for TypeScript/Node.js use\n\n👉 Set `DATASET_PATH` to your dataset root (direct parent of the group folders).\n","metadata":{}},{"cell_type":"code","source":"import os, pickle\nfrom pathlib import Path\nimport glob, random\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import TripletMarginLoss\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nimport pytorch_lightning as pl\nfrom transformers import AutoImageProcessor, AutoModel, ViTImageProcessor, ViTModel\nimport matplotlib.pyplot as plt\nimport glob\nfrom tqdm import tqdm  # for a progress bar\n\npl.seed_everything(42)\n\n# =============================================\n# 1) Fast, cached file indexing (class = immediate subfolder)\n# =============================================\ndef get_image_files(root_dir, batch_cache=10000):\n    print(f\"Scanning for image files in {root_dir} ...\")\n    exts = {'.jpg', '.jpeg', '.png'}\n    all_files = []\n\n    root_path = Path(root_dir)\n    for cls_dir in root_path.iterdir():\n        if not cls_dir.is_dir():\n            continue\n        for idx, f in enumerate(cls_dir.glob('**/*')):\n            if f.suffix.lower() in exts:\n                all_files.append(str(f))\n            # Periodic batch caching\n            if len(all_files) % batch_cache == 0:\n                print(f\"Indexed {len(all_files)} files so far...\")\n\n    print(f\"Done. Found {len(all_files)} files.\")\n    return all_files\n\ndef build_class_index(files, root_dir):\n    \"\"\"Group images by their immediate subfolder under root_dir\"\"\"\n    root = Path(root_dir)\n    class_to_images = {}\n    for f in files:\n        rel = Path(f).resolve().relative_to(root.resolve())\n        parts = rel.parts\n        if len(parts) < 2:\n            continue  # skip images directly in root\n        cls = parts[0]  # immediate subfolder name (e.g., \"AR 101\")\n        class_to_images.setdefault(cls, []).append(f)\n    # Keep only classes with >= 2 images (needed for triplets)\n    class_to_images = {k: v for k, v in class_to_images.items() if len(v) >= 2}\n\n    if len(class_to_images) == 0:\n        print(\"⚠️ No valid classes found! Checking dataset structure...\")\n        for dirpath, dirnames, filenames in os.walk(DATASET_PATH):\n            print(f\"{dirpath}: {len(filenames)} files\")\n            break  # only show top-level\n    else:\n        print(f\"✅ Found {len(class_to_images)} classes\")\n        for cls, imgs in list(class_to_images.items())[:5]:\n            print(f\"Class: {cls}, {len(imgs)} images, sample: {imgs[0]}\")\n    \n    return class_to_images\n\n# =============================================\n# 2) Dataset using pre-indexed class mapping\n# =============================================\nclass ComicTripletDataset(Dataset):\n    def __init__(self, class_to_images, transform=None):\n        self.transform = transform\n        self.class_to_images = class_to_images\n        self.classes = list(self.class_to_images.keys())\n\n    def __len__(self):\n        # You can scale this; here we do 10 * num_classes for balanced sampling\n        return max(len(self.classes) * 10, 1)\n\n    def __getitem__(self, idx):\n        anchor_class = random.choice(self.classes)\n        negative_class = random.choice([c for c in self.classes if c != anchor_class])\n\n        anchor_path, positive_path = random.sample(self.class_to_images[anchor_class], 2)\n        negative_path = random.choice(self.class_to_images[negative_class])\n\n        anchor_img = Image.open(anchor_path).convert(\"RGB\")\n        positive_img = Image.open(positive_path).convert(\"RGB\")\n        negative_img = Image.open(negative_path).convert(\"RGB\")\n\n        if self.transform:\n            anchor_img = self.transform(anchor_img)\n            positive_img = self.transform(positive_img)\n            negative_img = self.transform(negative_img)\n\n        return anchor_img, positive_img, negative_img\n\n# =============================================\n# 3) Model (ViT backbone + projection head) with loss tracking\n# =============================================\nclass ComicEmbeddingModel(pl.LightningModule):\n    def __init__(self, model_name=\"google/vit-base-patch16-224\", lr=2e-5, embed_dim=512):\n        super().__init__()\n        self.save_hyperparameters()\n        self.processor = ViTImageProcessor.from_pretrained(\n            model_name, \n            use_fast=True   # ✅ enable fast preprocessor\n        )\n        self.backbone = ViTModel.from_pretrained(model_name, add_pooling_layer=True)\n        self.fc = nn.Linear(self.backbone.config.hidden_size, embed_dim)\n        self.loss_fn = TripletMarginLoss(margin=0.2)\n        self.training_losses = []\n\n    def forward(self, x):\n        outputs = self.backbone(x)\n        pooled = outputs.pooler_output  # now this is a Tensor\n        embeddings = self.fc(pooled)\n        return F.normalize(embeddings, p=2, dim=1)\n\n    def training_step(self, batch, batch_idx):\n        anchor, positive, negative = batch\n        emb_a = self(anchor)\n        emb_p = self(positive)\n        emb_n = self(negative)\n        loss = self.loss_fn(emb_a, emb_p, emb_n)\n        self.training_losses.append(loss.item())\n        self.log(\"train_loss\", loss, prog_bar=True)\n        return loss\n\n    def configure_optimizers(self):\n        return torch.optim.AdamW(self.parameters(), lr=self.hparams.lr)\n\n# =============================================\n# 4) User-config: dataset path & caching\n# =============================================\nDATASET_PATH = \"/kaggle/input/inducks-entry-images/covers-by-storycode\"\nCACHE_DIR = \"/kaggle/working\" if Path('/kaggle/working').exists() else \".\"\nFILE_CACHE = os.path.join(CACHE_DIR, \"file_list.pkl\")\nCLASS_CACHE = os.path.join(CACHE_DIR, \"class_to_images.pkl\")\n\n# !rm -f /kaggle/working/file_list.pkl /kaggle/working/class_to_images.pkl\n\nif Path(CLASS_CACHE).exists():\n    with open(CLASS_CACHE, 'rb') as f:\n        class_to_images = pickle.load(f)\n    print(f\"Loaded class mapping from cache: {len(class_to_images)} classes\")\nelse:\n    if Path(FILE_CACHE).exists():\n        with open(FILE_CACHE, 'rb') as f:\n            all_files = pickle.load(f)\n        print(f\"Loaded {len(all_files)} file paths from cache\")\n    else:\n        print(\"Indexing files (first run may take a while)...\")\n        all_files = get_image_files(DATASET_PATH)\n        with open(FILE_CACHE, 'wb') as f:\n            pickle.dump(all_files, f)\n        print(f\"Indexed {len(all_files)} image files and cached to {FILE_CACHE}\")\n\n    class_to_images = build_class_index(all_files, DATASET_PATH)\n    with open(CLASS_CACHE, 'wb') as f:\n        pickle.dump(class_to_images, f)\n    print(f\"Built class mapping: {len(class_to_images)} classes cached to {CLASS_CACHE}\")\n\nnum_images = sum(len(v) for v in class_to_images.values())\nprint(f\"Ready: {len(class_to_images)} classes, {num_images} images with >=2 per class.\")\n\n# =============================================\n# 5) Transforms, Dataset, DataLoader (I/O-friendly settings)\n# =============================================\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n    transforms.RandomRotation(5),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n])\n\nmax_classes = 5000\nsampled_classes = dict(random.sample(list(class_to_images.items()), max_classes))\ndataset = ComicTripletDataset(sampled_classes, transform=transform)\n\n# Try modest workers to avoid oversubscribing I/O on Kaggle\nBATCH_SIZE = 16\nNUM_WORKERS = 2\nloader = DataLoader(\n    dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    num_workers=NUM_WORKERS,\n    pin_memory=True,\n    persistent_workers=True\n)\n\n# =============================================\n# 6) Trainer: single GPU by default, DDP optional\n# =============================================\nnum_gpus = torch.cuda.device_count()\nUSE_DDP = False  # Set to True AFTER you confirm fast startup with single GPU\n\nmodel = ComicEmbeddingModel(lr=2e-5)\n\nif num_gpus > 0:\n    accelerator = \"gpu\"\n    devices = 1   # ✅ stick to 1 GPU for Kaggle stability\n    strategy = \"auto\"   # \"ddp\" can break in notebooks, keep \"auto\" here\n    precision = 16\nelse:\n    accelerator = \"cpu\"\n    devices = 1\n    strategy = \"auto\"\n    precision = 32\n\ntrainer = pl.Trainer(\n    max_epochs=1,              # just 1 epoch per run\n    accelerator=accelerator,\n    devices=devices,\n    strategy=strategy,\n    precision=precision, \n    limit_train_batches=0.1,   # train on only 10% of batches\n    log_every_n_steps=5\n)\ntrainer.fit(model, loader)\n\nOUT_DIR = \"/kaggle/working\" if Path('/kaggle/working').exists() else \".\"\npt_path = os.path.join(OUT_DIR, \"comic_embedding_model.pt\")\ntorch.save(model.state_dict(), pt_path)\nprint(f\"Model saved to {pt_path}\")\n\ntrainer.save_checkpoint(f\"{OUT_DIR}/comic_model_epoch1.ckpt\")\n\n# =============================================\n# 7) Plot training loss\n# =============================================\nplt.figure(figsize=(8,5))\nplt.plot(model.training_losses, label=\"Training Loss\")\nplt.xlabel(\"Batch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training Loss Over Time\")\nplt.legend()\nplt.grid(True)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T16:10:04.659644Z","iopub.execute_input":"2025-08-16T16:10:04.659910Z","execution_failed":"2025-08-16T17:20:19.787Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================\n# 8) Export to ONNX for TypeScript/Node.js\n# =============================================\nimport torch\nfrom pathlib import Path\n\nOUT_DIR = \"/kaggle/working\" if Path('/kaggle/working').exists() else \".\"\npt_path = os.path.join(OUT_DIR, \"comic_embedding_model.pt\")\nonnx_path = os.path.join(OUT_DIR, \"comic_embedding_model.onnx\")\n\nexport_model = ComicEmbeddingModel()\nexport_model.load_state_dict(torch.load(pt_path, map_location=\"cpu\"))\nexport_model.eval()\n\ndummy_input = torch.randn(1, 3, 224, 224)\n\ntorch.onnx.export(\n    export_model,\n    dummy_input,\n    onnx_path,\n    input_names=[\"input\"],\n    output_names=[\"embedding\"],\n    dynamic_axes={\"input\": {0: \"batch\"}, \"embedding\": {0: \"batch\"}},\n    opset_version=17,\n)\nprint(f\"ONNX model saved to {onnx_path}\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-16T17:20:19.789Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Tips for very large datasets\n- Keep `USE_DDP=False` for the first successful run; turn it on only if startup is fast and stable.\n- Consider packing images into **WebDataset (.tar)** shards for faster sequential I/O.\n- Increase `BATCH_SIZE` if you have headroom (watch GPU memory).\n- You can reduce startup time further by saving `class_to_images.pkl` and reusing it across sessions.","metadata":{}}]}